# Automated Evals

This repo contains code to conduct automated evaluations of LLMs.


## Setup

Create a virtual environment and install the package in editable mode:

```bash
conda create -n automated-evals python=3.13
conda activate automated-evals
pip install -e .
```

## Repo structure

- `src/automated_evals/`: This is the main package for automated evals.
- `experiments/`: This is folder where experiment notebooks/scripts are stored.
See the README file in experiments for details.